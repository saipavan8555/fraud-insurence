# -*- coding: utf-8 -*-
"""fraud_insurence_claims

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1umJY6AHV7lhUizdL8Qnx65Kl9n1IPmOY

# IMPORTING LIBRARIES
"""

import pandas as pd       #(pandas for reading dataframe)
import numpy as np        #(numpy for mathematical operations)
pd.pandas.set_option('display.max_columns', None)  
import matplotlib.pyplot as plt     #(matplotlib for visualizations )
import seaborn as sns               #(seaborn for visualizations)
from sklearn.impute import SimpleImputer  #(to imputing null values)
from sklearn.preprocessing import StandardScaler    #(to scaling the numerical variables)
from sklearn.preprocessing import OneHotEncoder     #(to converting categorical variables into '1' & '0')
from sklearn.model_selection import train_test_split    #(to spliting given train data into 'train' & 'test')

from sklearn.linear_model import LogisticRegression 
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
import xgboost as xgb
from xgboost.sklearn import XGBClassifier
from sklearn.neighbors import KNeighborsClassifier

from sklearn.model_selection import GridSearchCV

from sklearn.metrics import classification_report
from sklearn.metrics import f1_score



"""# Reading Train Data:"""

demo=pd.read_csv("Train_Demographics.csv", na_values=['NA'])

#demo.head(3)
demo.shape

claim=pd.read_csv("Train_Claim.csv", na_values=['?','-5','MISSINGVALUE','MISSEDDATA'])

#claim.head(3)
claim.shape

policy=pd.read_csv("Train_Policy.csv", na_values=['-1','MISSINGVAL','NA'])

#policy.head(3)
policy.shape

vehicle= pd.read_csv("Train_Vehicle.csv", na_values=['???'])

#vehicle.head(3)

vehicle.shape

vehicle = vehicle.pivot(index="CustomerID", columns="VehicleAttribute", values="VehicleAttributeDetails")

vehicle.shape

#vehicle.head(3)

target= pd.read_csv("Traindata_with_Target.csv")

#target.head(3)
target.shape

"""# Merging Train DataSets :"""

train = demo.merge(claim, on="CustomerID")

#train.head(3)

train = train.merge(policy, on="CustomerID")

train.head(3)

train = train.merge(vehicle, on="CustomerID")

train.head(3)

train = train.merge(target, on="CustomerID")

train







test_demo=pd.read_csv("Test_Demographics.csv", na_values=['NA'])
test_claim=pd.read_csv("Test_Claim.csv", na_values=['?','-5','MISSINGVALUE','MISSEDDATA'])
test_policy=pd.read_csv("Test_Policy.csv", na_values=['-1','MISSINGVAL','NA'])
test_vehicle= pd.read_csv("Test_Vehicle.csv", na_values=['???'])
test_vehicle = test_vehicle.pivot(index="CustomerID", columns="VehicleAttribute", values="VehicleAttributeDetails")

test = test_demo.merge(test_claim, on="CustomerID")
test = test.merge(test_policy, on="CustomerID")
test = test.merge(test_vehicle, on="CustomerID")

test.shape







# To Check three is imbalance in Target column : 
train['ReportedFraud'].value_counts()

# To Check How Many Categories in 'Country' column : 
train['Country'].value_counts()

# To Check How Many Categories in 'InsuredHobbies' column : 
train['InsuredHobbies'].value_counts()

# To Check How Many Categories in 'IncidentState' column : 
train['IncidentState'].value_counts()

"""# Visualizations :"""

# ploting histogram 
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as py
sns.set(rc={'figure.figsize':(11.7,8.27)})
sns.histplot(binwidth=5,
            x=train['InsuredGender'],
            hue=train['ReportedFraud'],
            data=train,
            stat="count",
            multiple="dodge")

import matplotlib.pyplot as plt
import seaborn as sns

f, axes = plt.subplots(1, 6,figsize=(50,12))
sns.countplot(  x= "NumberOfVehicles",hue='ReportedFraud', data=train,  orient='v' , ax=axes[0])
sns.countplot(  x= "BodilyInjuries",hue='ReportedFraud', data=train,  orient='v' , ax=axes[1])
sns.countplot(   x= "TypeOfCollission",hue='ReportedFraud', data=train,  orient='v' , ax=axes[2])
sns.countplot(   x= "SeverityOfIncident",hue='ReportedFraud', data=train,  orient='v' , ax=axes[3])
sns.countplot(   x= "PropertyDamage",hue='ReportedFraud', data=train,  orient='v' , ax=axes[4])
sns.countplot(   x= "BodilyInjuries",hue='ReportedFraud', data=train,  orient='v' , ax=axes[5])

import matplotlib.pyplot as plt
import seaborn as sns

f, axes = plt.subplots(1, 3,figsize=(50,12))
sns.countplot(  x= "PoliceReport",hue='ReportedFraud', data=train,  orient='v' , ax=axes[0])
sns.countplot(  x= "Witnesses",hue='ReportedFraud', data=train,  orient='v' , ax=axes[1])
sns.countplot(   x= "InsurancePolicyState",hue='ReportedFraud', data=train,  orient='v' , ax=axes[2])



import matplotlib.pyplot as plt
import seaborn as sns

f, axes = plt.subplots(1, 4,figsize=(50,12))
sns.countplot(  x= "PoliceReport",hue='ReportedFraud', data=train,  orient='v' , ax=axes[0])
sns.countplot(  x= "Witnesses",hue='ReportedFraud', data=train,  orient='v' , ax=axes[1])
sns.countplot(   x= "InsurancePolicyState",hue='ReportedFraud', data=train,  orient='v' , ax=axes[2])
sns.countplot(   x= "TypeOfIncident",hue='ReportedFraud', data=train,  orient='v' , ax=axes[3])

fig = plt.figure(figsize=(30, 5))

sns.countplot(   x= "InsuredHobbies",hue='ReportedFraud', data=train)

train['InsuredHobbies']=train['InsuredHobbies'].apply(lambda x :'Other' if x!='chess' and x!='cross-fit' else x)

test['InsuredHobbies']=test['InsuredHobbies'].apply(lambda x :'Other' if x!='chess' and x!='cross-fit' else x)

train['InsuredHobbies'].value_counts(normalize=True)

f, axes = plt.subplots(1, 3,figsize=(50,12))
sns.countplot(  x= "InsuredRelationship",hue='ReportedFraud', data=train,  orient='v' , ax=axes[0])
sns.countplot(  x= "InsuredGender",hue='ReportedFraud', data=train,  orient='v' , ax=axes[1])
sns.countplot(  x= "IncidentTime",hue='ReportedFraud', data=train,  orient='v' , ax=axes[2])

f, axes = plt.subplots(1, 3,figsize=(50,12))
sns.countplot(  x= "IncidentCity",hue='ReportedFraud', data=train,  orient='v' , ax=axes[0])
sns.countplot(  x= "IncidentState",hue='ReportedFraud', data=train,  orient='v' , ax=axes[1])
sns.countplot(  x= "InsurancePolicyState",hue='ReportedFraud', data=train,  orient='v' , ax=axes[2])

train.columns

f, axes = plt.subplots(1, 3,figsize=(50,12))
sns.countplot(  x= "VehicleMake",hue='ReportedFraud', data=train,  orient='v' , ax=axes[0])
sns.countplot(  x= "VehicleModel",hue='ReportedFraud', data=train,  orient='v' , ax=axes[1])
sns.countplot(  x= "Policy_CombinedSingleLimit",hue='ReportedFraud', data=train,  orient='v' , ax=axes[2])

import matplotlib.pyplot as plt
# Create histogram
plt.hist(train['AmountOfTotalClaim'], bins=10)

plt.xlabel('AmountOfTotalClaim')
plt.ylabel('Frequency')

plt.title('Histogram Example')
plt.show()

# ploting histogram for two variables :
sns.histplot(x='AmountOfTotalClaim', data=train, kde=True, hue='ReportedFraud', bins=15)
plt.show()

f, axes = plt.subplots(1, 5,figsize=(40,7))
sns.histplot(  x= "CapitalLoss",hue='ReportedFraud', data=train,  kde=True, ax=axes[0])
sns.histplot(  x= "PolicyAnnualPremium",hue='ReportedFraud', data=train,  kde=True,  ax=axes[1])
sns.histplot(  x= "Policy_Deductible",hue='ReportedFraud', data=train,  kde=True,  ax=axes[2])
sns.histplot(  x= "Policy_CombinedSingleLimit",hue='ReportedFraud', data=train,  kde=True,  ax=axes[3])
sns.histplot(  x= "CustomerLoyaltyPeriod",hue='ReportedFraud', data=train,  kde=True,  ax=axes[4])

import plotly.express as py
sc_fig=py.scatter(train,x='CapitalGains',y='CapitalLoss',trendline="ols")
sc_fig.show()

sc_fig=py.scatter(train,x='AmountOfPropertyClaim',y='AmountOfInjuryClaim',trendline="ols")
sc_fig.show()



# To plot histogram :
sns.histplot(data=train, x="AmountOfTotalClaim")

train.columns

# To Drop columns from Dataset :
train.drop(['CustomerID','IncidentCity','InsuredZipCode','InsurancePolicyState','Country','IncidentAddress','InsurancePolicyNumber','VehicleID','DateOfIncident','DateOfPolicyCoverage'], axis=1, inplace=True)

# To Drop columns from Dataset :
test.drop(['CustomerID','IncidentCity','InsuredZipCode','InsurancePolicyState','Country','IncidentAddress','InsurancePolicyNumber','VehicleID','DateOfIncident','DateOfPolicyCoverage'], axis=1, inplace=True)

#ploting heatmap to know corelation between numerical variables :
import matplotlib.pyplot as plt
plt.figure(figsize=(13,10))
sns.heatmap(data=train.corr(), annot=True,fmt='.2g',linewidth=1)
plt.show()

#ploting scatter plot to know corelation between only two numerical variables :
sc_fig=py.scatter(train,x='AmountOfPropertyClaim',y='AmountOfInjuryClaim',trendline="ols")
sc_fig.show()

"""# Droping Unwanted Columns :"""

# AmountOfTotalClaim is sum of the AmountOfPropertyClaim,AmountOfInjuryClaim,AmountOfVehicleDamage
train['AmountOfTotalClaim']=train['AmountOfPropertyClaim']+train['AmountOfInjuryClaim']+train['AmountOfVehicleDamage']

# AmountOfTotalClaim is sum of the AmountOfPropertyClaim,AmountOfInjuryClaim,AmountOfVehicleDamage
test['AmountOfTotalClaim']=test['AmountOfPropertyClaim']+test['AmountOfInjuryClaim']+test['AmountOfVehicleDamage']

# To Drop columns from Dataset :
train.drop(['AmountOfVehicleDamage','AmountOfInjuryClaim','AmountOfPropertyClaim','InsuredAge'], axis=1, inplace=True)

# To Drop columns from Dataset :
test.drop(['AmountOfVehicleDamage','AmountOfInjuryClaim','AmountOfPropertyClaim','InsuredAge'], axis=1, inplace=True)









train.describe()



#to plot boxplot to show the outliers :
import matplotlib.pyplot as plt
train.plot(kind='box',subplots=True,layout=(7,7),figsize=(30,30))
plt.show()





train.head(3)

train.dtypes

train.nunique()

"""# Data Type Conversion :"""

# converting data type to 'object' :
train['NumberOfVehicles'] = train['NumberOfVehicles'].astype('object')

test['NumberOfVehicles'] = test['NumberOfVehicles'].astype('object')

train['Witnesses'] = train['Witnesses'].astype('object')

test['Witnesses'] = test['Witnesses'].astype('object')

train['BodilyInjuries'] = train['BodilyInjuries'].astype('object')

test['BodilyInjuries'] = test['BodilyInjuries'].astype('object')

train['IncidentTime'] = train['IncidentTime'].astype('object')

test['IncidentTime'] = test['IncidentTime'].astype('object')

train['IncidentTime'].value_counts()

train.dtypes

train.shape

test.dtypes

test.shape





train.shape

train.head()

train['InsuredGender'].value_counts()





train.dtypes

"""# Handling Outliers :"""

# by getting outliers to interquartile range :

data=train
# Calculate the quartiles and IQR for each column
q1 = data.quantile(0.25)
q3 = data.quantile(0.75)
iqr = q3 - q1

# Identify outliers in each column
outliers = (data < (q1 - 1.5 * iqr)) | (data > (q3 + 1.5 * iqr))

# Create a new DataFrame to show the location of outliers
outliers_df = pd.DataFrame(index=data.index, columns=data.columns, data=outliers.astype(int))



# Count the number of outliers
num_outliers = outliers.sum()

print("**Number of outliers in each column :")
print('')
print(num_outliers)

num_cols=train.select_dtypes(include=['int64','float64']).columns

# Replace outliers with the nearest value within the IQR range
for column in num_cols:
    lower_bound = q1[column] - 1.5 * iqr[column]
    upper_bound = q3[column] + 1.5 * iqr[column]
    train[column] = train[column].apply(lambda x: min(max(x, lower_bound), upper_bound))

data=train
# Calculate the quartiles and IQR for each column
q1 = data.quantile(0.25)
q3 = data.quantile(0.75)
iqr = q3 - q1

# Identify outliers in each column
outliers = (data < (q1 - 1.5 * iqr)) | (data > (q3 + 1.5 * iqr))

# Create a new DataFrame to show the location of outliers
outliers_df = pd.DataFrame(index=data.index, columns=data.columns, data=outliers.astype(int))

# Count the number of outliers
num_outliers = outliers.sum()

print("**Number of outliers in each column :")
print('')
print(num_outliers)

# Drawing the boxplot to identify the presence of outliers in train data: 
train.plot(kind ='box',subplots = True,sharex= False,sharey=False,layout=(4,4),figsize=(10,10))
plt.show()





# by getting outliers to interquartile range :

data=test
# Calculate the quartiles and IQR for each column
q1 = data.quantile(0.25)
q3 = data.quantile(0.75)
iqr = q3 - q1

# Identify outliers in each column
outliers = (data < (q1 - 1.5 * iqr)) | (data > (q3 + 1.5 * iqr))

# Create a new DataFrame to show the location of outliers
outliers_df = pd.DataFrame(index=data.index, columns=data.columns, data=outliers.astype(int))

# Count the number of outliers
num_outliers = outliers.sum()

print("**Number of outliers in each column :")
print('')
print(num_outliers)

num_cols=test.select_dtypes(include=['int64','float64']).columns

# Replace outliers with the nearest value within the IQR range
for column in num_cols:
    lower_bound = q1[column] - 1.5 * iqr[column]
    upper_bound = q3[column] + 1.5 * iqr[column]
    test[column] = test[column].apply(lambda x: min(max(x, lower_bound), upper_bound))

data=test
# Calculate the quartiles and IQR for each column
q1 = data.quantile(0.25)
q3 = data.quantile(0.75)
iqr = q3 - q1

# Identify outliers in each column
outliers = (data < (q1 - 1.5 * iqr)) | (data > (q3 + 1.5 * iqr))

# Create a new DataFrame to show the location of outliers
outliers_df = pd.DataFrame(index=data.index, columns=data.columns, data=outliers.astype(int))

# Count the number of outliers
num_outliers = outliers.sum()

print("**Number of outliers in each column :")
print('')
print(num_outliers)

# Drawing the boxplot to identify the presence of outliers in train data: 
test.plot(kind ='box',subplots = True,sharex= False,sharey=False,layout=(4,4),figsize=(10,10))
plt.show()



"""# Handling Null Values :"""

train.isnull().sum()

train.isnull().sum().sum()

train['TypeOfCollission'].unique() # missing value represented as "?"

train['PropertyDamage'].unique()  # missing value represented as "?"

train['PoliceReport'].unique()    # missing value represented as "?"









#train1=train.copy()
#train1['PoliceReport']= train1['PoliceReport'].fillna('other_PoliceReport')



train.dtypes

num_cols=train.select_dtypes(include=['int64','float64']).columns

num_cols

cat_cols_test=test.select_dtypes(include=['object']).columns
cat_cols_test.shape



cat_cols=train.select_dtypes(include=['object']).columns

cat_cols.shape

train[cat_cols]=train[cat_cols].astype('category')

test[cat_cols_test]=test[cat_cols_test].astype('category')

train.dtypes

train['ReportedFraud'] = train['ReportedFraud'].apply(lambda x: 1 if x == "Y" else 0)

train['ReportedFraud'].value_counts(normalize=True)

# spliting given train dataset into independent variables(x) & dependend variable(y) :
X=train.drop('ReportedFraud',axis=1)
y=train['ReportedFraud']

from sklearn.model_selection import train_test_split
X_original_train, X_original_test, y_original_train, y_original_test = train_test_split(X,y,test_size=0.3, random_state=123)

print(X_original_train.shape)
print(y_original_train.shape)
print(X_original_test.shape)
print(y_original_test.shape)

X_original_train.dtypes

cat_cols_x_train=X_original_train.select_dtypes(include=['category']).columns
cat_cols_x_test=X_original_test.select_dtypes(include=['category']).columns
cat_cols_test=test.select_dtypes(include=['category']).columns

cat_cols_x_test

X_original_test['PropertyDamage'] = X_original_test['PropertyDamage'].apply(lambda x: 1 if x == "YES" else 0)
X_original_test['PoliceReport'] = X_original_test['PoliceReport'].apply(lambda x: 1 if x == "YES" else 0)
X_original_train['PropertyDamage'] = X_original_train['PropertyDamage'].apply(lambda x: 1 if x == "YES" else 0)
X_original_train['PoliceReport'] = X_original_train['PoliceReport'].apply(lambda x: 1 if x == "YES" else 0)
test['PropertyDamage'] = test['PropertyDamage'].apply(lambda x: 1 if x == "YES" else 0)
test['PoliceReport'] = test['PoliceReport'].apply(lambda x: 1 if x == "YES" else 0)

X_original_test['PoliceReport'].value_counts()

X_original_train.isna().sum()

knn_cols=['PoliceReport','PropertyDamage']
from sklearn.impute import KNNImputer
imputer = KNNImputer(n_neighbors=5)
X_original_train[knn_cols] = imputer.fit_transform(X_original_train[knn_cols])
X_original_test[knn_cols] = imputer.transform(X_original_test[knn_cols])
test[knn_cols] = imputer.transform(test[knn_cols])

X_original_train.isna().sum()

test.isna().sum()

#Imputing Categorical null values by using mode :
cat_imp = SimpleImputer(strategy= 'most_frequent')
X_original_train[cat_cols_x_train]=cat_imp.fit_transform(X_original_train[cat_cols_x_train])
X_original_test[cat_cols_x_test]=cat_imp.transform(X_original_test[cat_cols_x_test])
test[cat_cols_test]=cat_imp.transform(test[cat_cols_test])

num_cols_x_train=X_original_train.select_dtypes(include=['int','float']).columns
num_cols_x_test=X_original_test.select_dtypes(include=['int','float']).columns
num_cols_test=test.select_dtypes(include=['int','float']).columns

num_cols_x_train

num_cols_test

#Imputing numerical null values by using median :
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy='median')
X_original_train[num_cols_x_train]=cat_imp.fit_transform(X_original_train[num_cols_x_train])
X_original_test[num_cols_x_test]=cat_imp.transform(X_original_test[num_cols_x_test])
test[num_cols_test]=cat_imp.transform(test[num_cols_test])

X_original_train.isnull().sum()

test.isna().sum()



"""# X, Y split :"""

def pie_chat(data,df,cols=None):
  data_name=data[cols].value_counts().index
  data_value=data[cols].value_counts().values

   #"df refers to how many number of classes do you want to show"
   #used for one categorical

  plt.pie(data_value[:df],labels=data_name[:df],autopct="%1.2f%%")

pie_chat(train,10,cols='ReportedFraud')



















#from sklearn.impute import SimpleImputer

#cat_imp = SimpleImputer(strategy= 'most_frequent')
#x[cat_cols]=cat_imp.fit_transform(x[cat_cols])

#x.isnull().sum()

#x[num_cols] = x[num_cols].fillna(x[num_cols].median()[0])
#from sklearn.impute import SimpleImputer
#imputer = SimpleImputer(strategy='median')
    
#imputer = imputer.fit(x[num_cols])
#x[num_cols] = imputer.transform(x[num_cols])

num_cols_x_test

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaler.fit(X_original_train[num_cols_x_train])
X_original_train[num_cols_x_train]=scaler.transform(X_original_train[num_cols_x_train])
X_original_test[num_cols_x_test]=scaler.transform(X_original_test[num_cols_x_test])
test[num_cols_test]=scaler.transform(test[num_cols_test])

X_original_train

test

X_original_train.dtypes

x_train_cat_cols=X_original_train.select_dtypes(['object']).columns
x_test_cat_cols=X_original_test.select_dtypes(['object']).columns
test_cat_cols=test.select_dtypes(['object']).columns

X_original_train[x_train_cat_cols]=X_original_train[x_train_cat_cols].astype('category')
X_original_test[x_test_cat_cols]=X_original_test[x_test_cat_cols].astype('category')
test[test_cat_cols]=test[test_cat_cols].astype('category')

X_original_train.dtypes

x_train_cat=X_original_train.select_dtypes(['category']).columns
x_test_cat=X_original_test.select_dtypes(['category']).columns
test_cat=test.select_dtypes(['category']).columns

x_train_cat.shape

test_cat.shape

#from sklearn.preprocessing import OneHotEncoder

#you tube onehot encoding
from sklearn.preprocessing import OneHotEncoder

ohe=OneHotEncoder(handle_unknown='ignore',drop= 'first')
ohe=ohe.fit(test[test_cat])

col_ohe_x_train= list(ohe.get_feature_names_out(x_train_cat))

col_ohe_test= list(ohe.get_feature_names_out(test_cat))

col_ohe_x_test= list(ohe.get_feature_names_out(x_test_cat))

cat_cols_x_train=ohe.transform(X_original_train[x_train_cat])

cat_cols_x_test=ohe.transform(X_original_test[x_test_cat])

cat_cols_test=ohe.transform(test[test_cat])

cat_cols_x_train=pd.DataFrame(cat_cols_x_train.todense(),columns=col_ohe_x_train)

cat_cols_x_test=pd.DataFrame(cat_cols_x_test.todense(),columns=col_ohe_x_test)

cat_cols_test=pd.DataFrame(cat_cols_test.todense(),columns=col_ohe_test)

cat_cols_x_train

cat_cols_test

print(X_original_train.shape)
print(cat_cols_x_train.shape)

train_num=X_original_train.select_dtypes(include=['int','float64'])
x_test_num=X_original_test.select_dtypes(include=['int','float'])
test_num=test.select_dtypes(include=['int','float'])

train_num.reset_index(inplace = True, drop = True)
x_test_num.reset_index(inplace = True, drop = True)
test_num.reset_index(inplace = True, drop = True)

X_original_train = pd.concat([cat_cols_x_train,train_num],axis='columns')
X_original_train

X_original_test = pd.concat([cat_cols_x_test,x_test_num],axis='columns')
X_original_test

test = pd.concat([cat_cols_test,test_num],axis='columns')
test

print(test.shape)
print(X_original_test.shape)
print(X_original_train.shape)





"""# upsampling"""

pip install imbalanced-learn

from imblearn.over_sampling import SMOTE
smote = SMOTE(random_state=123)
X_smote_train, y_smote_train= smote.fit_resample(X_original_train, y_original_train)

X_original_train.shape

X_smote_train.shape

y_original_train.value_counts()

y_smote_train.value_counts()

# function for ploting learning curves:
def plot_learning_curves(X_train, y_train, X_test, y_test, estimator):
    train_sizes, train_scores, test_scores = learning_curve(
        estimator, X_train, y_train, train_sizes=np.linspace(0.1, 1.0, 10),
        scoring=make_scorer(f1_score, average='weighted'), cv=5)
    
    train_scores_mean = np.mean(train_scores, axis=1)
    train_scores_std = np.std(train_scores, axis=1)
    test_scores_mean = np.mean(test_scores, axis=1)
    test_scores_std = np.std(test_scores, axis=1)

    plt.figure()
    plt.title('Learning Curves')
    plt.xlabel("Training examples")
    plt.ylabel("Score")

    plt.grid()
    
    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,
                     train_scores_mean + train_scores_std, alpha=0.1, color="r")
    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,
                     test_scores_mean + test_scores_std, alpha=0.1, color="g")
    plt.plot(train_sizes, train_scores_mean, 'o-', color="r",
             label="Training score")
    plt.plot(train_sizes, test_scores_mean, 'o-', color="g",
             label="Cross-validation score")

    plt.legend(loc="best")
    plt.show()

from sklearn import metrics
import numpy as np
Metrics = pd.DataFrame(columns=['Model','F1_Train','F1_Test'])
#definition of error metrics function
def get_metrics(X_train,y_pred_train,X_test,y_pred_test,model_description,Metrics):
    F1_Train = metrics.f1_score(X_train,y_pred_train,average="weighted")
    F1_Test = metrics.f1_score(X_test,y_pred_test,average="weighted")
    Metrics=Metrics.append(pd.Series([model_description,F1_Train,F1_Test],
                                           index=Metrics.columns),
                               ignore_index=True)
    return(Metrics)

"""# Logistic Regression"""

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB

from sklearn.metrics import classification_report
from sklearn.metrics import f1_score

lr=LogisticRegression()


X_train=X_original_train
X_test=X_original_test
y_train=y_original_train
y_test=y_original_test


lr.fit(X_train,y_train)

y_pred_train_lr=lr.predict(X_train)

y_pred_test_lr=lr.predict(X_test)

train_acc=lr.score(X_train,y_train)

test_acc=lr.score(X_test,y_test)

f1_score_train=f1_score(y_train,y_pred_train_lr,average='weighted')

f1_score_test=f1_score(y_test,y_pred_test_lr,average='weighted')

# Generate the classification report
report_train = classification_report(y_train, y_pred_train_lr)


# Generate the classification report
report_test = classification_report(y_test, y_pred_test_lr)


print('Train accuracy : ',train_acc)
print('')

print('Test accuracy : ',test_acc)
print('')

print('F1_score of Train data :',f1_score_train)
print('')

print('F1_score of Test data :',f1_score_test)
print('')
print('--------------------** TRAIN REPORT **----------------------------')
print('')
print('CLASSIFICATION REPORT TRAIN DATA :')
print('')
print(report_train)
print('--------------------** TEST REPORT **----------------------------')
print('')

print('CLASSIFICATION REPORT TEST DATA :')
print('')
print(report_test)
print('')
print('-----------------------------------------------------------------')

test_pred=lr.predict(test)

test_data = test_demo.merge(test_claim, on="CustomerID")
test_data = test_data.merge(test_policy, on="CustomerID")
test_data= test_data.merge(test_vehicle, on="CustomerID")

test_data.shape

test_data['ReportedFraud']=test_pred

test_data

test_data['CustomerID'] = test_data['CustomerID'].astype(str)
test_data = test_data[['CustomerID', 'ReportedFraud']]
test_data = test_data.rename(columns={'CustomerID': 'CustomerID', 'ReportedFraud': 'ReportedFraud'})
test_data

test_data['ReportedFraud'].value_counts()

#test_data.ReportedFraud = test_data.ReportedFraud.map({0 : 'N', 1 : 'Y'})

test_data['ReportedFraud'].value_counts()

test_data.to_csv('_lr_submission_.csv', index=False)

"""# Confusion Matrix"""

from sklearn.metrics import confusion_matrix

# generate confusion matrix
cm = confusion_matrix(y_pred_test_lr,y_test)
print(cm)

"""# Receiver Operating Characteristic (ROC) curve:"""

from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

# generate ROC curve
fpr, tpr, thresholds = roc_curve(y_test,y_pred_test_lr)
roc_auc = auc(fpr, tpr)

# plot the ROC curve and AUC percentage
plt.plot(fpr, tpr, label='ROC curve (AUC = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

"""# learning curves:"""

pip install mlxtend

from sklearn.model_selection import learning_curve
from mlxtend.plotting import plot_learning_curves #import leqrning curves

plot_learning_curves(X_train, y_train, X_test, y_test,lr,scoring="f1")#plotting learning curves
plt.show()

#from sklearn.metrics import f1_score, make_scorer
#plot_learning_curves(X_train, y_train, X_test, y_test, lr)

Metrics=get_metrics(y_train,y_pred_train_lr,y_test,y_pred_test_lr,'Logistic Regression',Metrics)
Metrics



"""# with smote (LR)"""

lr=LogisticRegression()

X_train=X_smote_train
X_test=X_original_test
y_train=y_smote_train
y_test=y_original_test


lr.fit(X_train,y_train)

y_pred_train_lr_sm=lr.predict(X_train)

y_pred_test_lr_sm=lr.predict(X_test)

train_acc=lr.score(X_train,y_train)

test_acc=lr.score(X_test,y_test)

f1_score_train=f1_score(y_train,y_pred_train_lr_sm, average='weighted')

f1_score_test=f1_score(y_test,y_pred_test_lr_sm, average='weighted')

# Generate the classification report
report_train = classification_report(y_train, y_pred_train_lr_sm)


# Generate the classification report
report_test = classification_report(y_test, y_pred_test_lr_sm)


print('Train accuracy : ',train_acc)
print('')

print('Test accuracy : ',test_acc)
print('')

print('F1_score of Train data :',f1_score_train)
print('')

print('F1_score of Test data :',f1_score_test)
print('')
print('--------------------** TRAIN REPORT **----------------------------')
print('')
print('CLASSIFICATION REPORT TRAIN DATA :')
print('')
print(report_train)
print('--------------------** TEST REPORT **----------------------------')
print('')

print('CLASSIFICATION REPORT TEST DATA :')
print('')
print(report_test)
print('')
print('-----------------------------------------------------------------')

from sklearn.metrics import confusion_matrix

# generate confusion matrix
cm = confusion_matrix(y_pred_test_lr_sm,y_test)
print(cm)

# generate ROC curve
fpr, tpr, thresholds = roc_curve(y_test,y_pred_test_lr_sm)
roc_auc = auc(fpr, tpr)

# plot the ROC curve and AUC percentage
plt.plot(fpr, tpr, label='ROC curve (AUC = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

Metrics=get_metrics(y_train,y_pred_train_lr_sm,y_test,y_pred_test_lr_sm,'Logistic Regression SMOTE',Metrics)
Metrics



"""#  GridSearchCV (LR)"""

from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report


X_train=X_original_train
X_test=X_original_test
y_train=y_original_train
y_test=y_original_test


# defining the hyperparameters to tune
param_grid = {'penalty': ['l1', 'l2', 'elasticnet', 'none'],
              'C': [0.1, 0.5, 1.0, 2.0]}

# using GridSearchCV to find the best hyperparameters
grid_search = GridSearchCV(lr, param_grid, cv=5)
grid_search.fit(X_train, y_train)

# printing the best hyperparameters
print("Best hyperparameters: ", grid_search.best_params_)
lr_params=LogisticRegression(**grid_search.best_params_)
lr_params.fit(X_train, y_train)

y_pred_train_lr_gs= grid_search.predict(X_train)

y_pred_test_lr_gs= grid_search.predict(X_test)

train_acc = grid_search.score(X_train,y_train)

test_acc = grid_search.score(X_test,y_test)

f1_score_train = f1_score(y_train,y_pred_train_lr_gs, average='weighted')

f1_score_test = f1_score(y_test,y_pred_test_lr_gs, average='weighted')

# Generate the classification report
report_train = classification_report(y_train, y_pred_train_lr_gs)


# Generate the classification report
report_test = classification_report(y_test, y_pred_test_lr_gs)


print('Train accuracy : ',train_acc)
print('')

print('Test accuracy : ',test_acc)
print('')

print('F1_score of Train data :',f1_score_train)
print('')

print('F1_score of Test data :',f1_score_test)
print('')
print('--------------------** TRAIN REPORT **----------------------------')
print('')
print('CLASSIFICATION REPORT TRAIN DATA :')
print('')
print(report_train)
print('--------------------** TEST REPORT **----------------------------')
print('')

print('CLASSIFICATION REPORT TEST DATA :')
print('')
print(report_test)
print('')
print('-----------------------------------------------------------------')

from sklearn.metrics import confusion_matrix

# generate confusion matrix
cm = confusion_matrix(y_pred_test_lr_gs,y_test)
print(cm)

# generate ROC curve
fpr, tpr, thresholds = roc_curve(y_test,y_pred_test_lr_gs)
roc_auc = auc(fpr, tpr)

# plot the ROC curve and AUC percentage
plt.plot(fpr, tpr, label='ROC curve (AUC = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

Metrics=get_metrics(y_train,y_pred_train_lr_gs,y_test,y_pred_test_lr_gs,'Logistic Regression GridSearch',Metrics)
Metrics



"""# #  GridSearchCV With SMOTE (LR)"""

from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report


X_train=X_smote_train
X_test=X_original_test
y_train=y_smote_train
y_test=y_original_test


# defining the hyperparameters to tune
param_grid = {'penalty': ['l1', 'l2', 'elasticnet', 'none'],
              'C': [0.1, 0.5, 1.0, 2.0]}

# using GridSearchCV to find the best hyperparameters
grid_search = GridSearchCV(lr, param_grid, cv=5)
grid_search.fit(X_train, y_train)

# printing the best hyperparameters
print("Best hyperparameters: ", grid_search.best_params_)
lr_params=LogisticRegression(**grid_search.best_params_)
lr_params.fit(X_train, y_train)

lr=LogisticRegression()
X_train=X_smote_train
y_train=y_smote_train
lr.fit(X_train,y_train)

y_pred_train_lr_gs_sm=grid_search.predict(X_train)

y_pred_test_lr_gs_sm=grid_search.predict(X_test)

train_acc=grid_search.score(X_train,y_train)

test_acc=grid_search.score(X_test,y_test)

f1_score_train=f1_score(y_train,y_pred_train_lr_gs_sm, average='weighted')

f1_score_test=f1_score(y_test,y_pred_test_lr_gs_sm, average='weighted')

# Generate the classification report
report_train = classification_report(y_train, y_pred_train_lr_gs_sm)


# Generate the classification report
report_test = classification_report(y_test, y_pred_test_lr_gs_sm)


print('Train accuracy : ',train_acc)
print('')

print('Test accuracy : ',test_acc)
print('')

print('F1_score of Train data :',f1_score_train)
print('')

print('F1_score of Test data :',f1_score_test)
print('')
print('--------------------** TRAIN REPORT **----------------------------')
print('')
print('CLASSIFICATION REPORT TRAIN DATA :')
print('')
print(report_train)
print('--------------------** TEST REPORT **----------------------------')
print('')

print('CLASSIFICATION REPORT TEST DATA :')
print('')
print(report_test)
print('')
print('-----------------------------------------------------------------')

cm = confusion_matrix(y_pred_test_lr_gs_sm,y_test)
print(cm)

# generate ROC curve
fpr, tpr, thresholds = roc_curve(y_test,y_pred_test_lr_gs_sm)
roc_auc = auc(fpr, tpr)

# plot the ROC curve and AUC percentage
plt.plot(fpr, tpr, label='ROC curve (AUC = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

Metrics=get_metrics(y_train,y_pred_train_lr_gs_sm,y_test,y_pred_test_lr_gs_sm,'Logistic Regression GridSearch SMOTE',Metrics)
Metrics



"""# DECESIONTREE CLASSIFIER"""

dt=DecisionTreeClassifier()

X_train=X_original_train
X_test=X_original_test
y_train=y_original_train
y_test=y_original_test

dt.fit(X_train,y_train)

y_pred_train_dt = dt.predict(X_train)

y_pred_test_dt = dt.predict(X_test)

train_acc = dt.score(X_train,y_train)

test_acc = dt.score(X_test,y_test)

f1_score_train=f1_score(y_train,y_pred_train_dt, average='weighted')

f1_score_test=f1_score(y_test,y_pred_test_dt, average='weighted')

# Generate the classification report
report_train = classification_report(y_train, y_pred_train_dt)


# Generate the classification report
report_test = classification_report(y_test, y_pred_test_dt)


print('Train accuracy : ',train_acc)
print('')

print('Test accuracy : ',test_acc)
print('')

print('F1_score of Train data :',f1_score_train)
print('')

print('F1_score of Test data :',f1_score_test)
print('')
print('--------------------** TRAIN REPORT **----------------------------')
print('')
print('CLASSIFICATION REPORT TRAIN DATA :')
print('')
print(report_train)
print('--------------------** TEST REPORT **----------------------------')
print('')

print('CLASSIFICATION REPORT TEST DATA :')
print('')
print(report_test)
print('')
print('-----------------------------------------------------------------')

cm = confusion_matrix(y_pred_test_dt,y_test)
print(cm)

# generate ROC curve
fpr, tpr, thresholds = roc_curve(y_test,y_pred_test_dt)
roc_auc = auc(fpr, tpr)

# plot the ROC curve and AUC percentage
plt.plot(fpr, tpr, label='ROC curve (AUC = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

Metrics=get_metrics(y_train,y_pred_train_dt,y_test,y_pred_test_dt,'DecisionTree',Metrics)
Metrics



"""#  with SMOTE (DT)"""

dt=DecisionTreeClassifier()

X_train=X_smote_train
X_test=X_original_test
y_train=y_smote_train
y_test=y_original_test

dt.fit(X_train,y_train)

y_pred_train_dt_sm = dt.predict(X_train)
y_pred_test_dt_sm = dt.predict(X_test)

train_acc=dt.score(X_train,y_train)
test_acc=dt.score(X_test,y_test)

f1_score_train=f1_score(y_train,y_pred_train_dt_sm, average='weighted')

f1_score_test=f1_score(y_test,y_pred_test_dt_sm, average='weighted')

# Generate the classification report
report_train = classification_report(y_train, y_pred_train_dt_sm)


# Generate the classification report
report_test = classification_report(y_test, y_pred_test_dt_sm)


print('Train accuracy : ',train_acc)
print('')

print('Test accuracy : ',test_acc)
print('')

print('F1_score of Train data :',f1_score_train)
print('')

print('F1_score of Test data :',f1_score_test)
print('')
print('--------------------** TRAIN REPORT **----------------------------')
print('')
print('CLASSIFICATION REPORT TRAIN DATA :')
print('')
print(report_train)
print('--------------------** TEST REPORT **----------------------------')
print('')

print('CLASSIFICATION REPORT TEST DATA :')
print('')
print(report_test)
print('')
print('-----------------------------------------------------------------')

cm = confusion_matrix(y_pred_test_dt_sm,y_test)
print(cm)

# generate ROC curve
fpr, tpr, thresholds = roc_curve(y_test,y_pred_test_dt_sm)
roc_auc = auc(fpr, tpr)

# plot the ROC curve and AUC percentage
plt.plot(fpr, tpr, label='ROC curve (AUC = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

Metrics=get_metrics(y_train,y_pred_train_dt_sm,y_test,y_pred_test_dt_sm,'DecisionTree SMOTE',Metrics)
Metrics

"""# GridSearch CV - (DT)"""

dt=DecisionTreeClassifier()

X_train=X_original_train
X_test=X_original_test
y_train=y_original_train
y_test=y_original_test

param_grid = {'criterion': ['gini', 'entropy'],
              'max_depth': [2, 4, 6, 8, 10,15,20,25],
              'min_samples_split': [2, 5, 10],
              'min_samples_leaf': [1, 2, 4]}

# using GridSearchCV to find the best hyperparameters
grid_search = GridSearchCV(dt, param_grid, cv=5)
grid_search.fit(X_train, y_train)

# printing the best hyperparameters
print("Best hyperparameters: ", grid_search.best_params_)
dt_params=DecisionTreeClassifier(**grid_search.best_params_)
dt_params.fit(X_train, y_train)

y_pred_train_dt_gs = dt_params.predict(X_train)
y_pred_test_dt_gs = dt_params.predict(X_test)

train_acc=dt_params.score(X_train,y_train)
test_acc=dt_params.score(X_test,y_test)

f1_score_train=f1_score(y_train,y_pred_train_dt_gs, average='weighted')

f1_score_test=f1_score(y_test,y_pred_test_dt_gs, average='weighted')

# Generate the classification report
report_train = classification_report(y_train, y_pred_train_dt_gs)


# Generate the classification report
report_test = classification_report(y_test, y_pred_test_dt_gs)


print('Train accuracy : ',train_acc)
print('')

print('Test accuracy : ',test_acc)
print('')

print('F1_score of Train data :',f1_score_train)
print('')

print('F1_score of Test data :',f1_score_test)
print('')
print('--------------------** TRAIN REPORT **----------------------------')
print('')
print('CLASSIFICATION REPORT TRAIN DATA :')
print('')
print(report_train)
print('--------------------** TEST REPORT **----------------------------')
print('')

print('CLASSIFICATION REPORT TEST DATA :')
print('')
print(report_test)
print('')
print('-----------------------------------------------------------------')

cm = confusion_matrix(y_pred_test_dt_gs,y_test)
print(cm)

# generate ROC curve
fpr, tpr, thresholds = roc_curve(y_test,y_pred_test_dt_gs)
roc_auc = auc(fpr, tpr)

# plot the ROC curve and AUC percentage
plt.plot(fpr, tpr, label='ROC curve (AUC = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

Metrics=get_metrics(y_train,y_pred_train_dt_gs,y_test,y_pred_test_dt_gs,'DecisionTree GridSearch',Metrics)
Metrics



"""#  GridSearchCV With SMOTE (DT)"""



#from sklearn.model_selection import GridSearchCV
#from sklearn.metrics import classification_report


X_train=X_smote_train
X_test=X_original_test
y_train=y_smote_train
y_test=y_original_test


param_grid = {'criterion': ['gini', 'entropy'],
              'max_depth': [2, 4, 6, 8, 10, None],
              'min_samples_split': [2, 5, 10],
              'min_samples_leaf': [1, 2, 4]}

# using GridSearchCV to find the best hyperparameters
grid_search = GridSearchCV(dt, param_grid, cv=5)
grid_search.fit(X_train, y_train)

# printing the best hyperparameters
print("Best hyperparameters: ", grid_search.best_params_)
dt_params=DecisionTreeClassifier(**grid_search.best_params_)
dt_params.fit(X_train, y_train)

dt=DecisionTreeClassifier()
X_train=X_smote_train
y_train=y_smote_train
dt.fit(X_train,y_train)

y_pred_train_dt_gs_sm = grid_search.predict(X_train)
y_pred_test_dt_gs_sm = grid_search.predict(X_test)

train_acc=grid_search.score(X_train,y_train)
test_acc=grid_search.score(X_test,y_test)

f1_score_train=f1_score(y_train,y_pred_train_dt_gs_sm, average='weighted')

f1_score_test=f1_score(y_test,y_pred_test_dt_gs_sm, average='weighted')

# Generate the classification report
report_train = classification_report(y_train, y_pred_train_dt_gs_sm)


# Generate the classification report
report_test = classification_report(y_test, y_pred_test_dt_gs_sm)


print('Train accuracy : ',train_acc)
print('')

print('Test accuracy : ',test_acc)
print('')

print('F1_score of Train data :',f1_score_train)
print('')

print('F1_score of Test data :',f1_score_test)
print('')
print('--------------------** TRAIN REPORT **----------------------------')
print('')
print('CLASSIFICATION REPORT TRAIN DATA :')
print('')
print(report_train)
print('--------------------** TEST REPORT **----------------------------')
print('')

print('CLASSIFICATION REPORT TEST DATA :')
print('')
print(report_test)
print('')
print('-----------------------------------------------------------------')

cm = confusion_matrix(y_pred_test_dt_gs_sm,y_test)
print(cm)

# generate ROC curve
fpr, tpr, thresholds = roc_curve(y_test,y_pred_test_dt_gs_sm)
roc_auc = auc(fpr, tpr)

# plot the ROC curve and AUC percentage
plt.plot(fpr, tpr, label='ROC curve (AUC = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

Metrics=get_metrics(y_train,y_pred_train_dt_gs_sm,y_test,y_pred_test_dt_gs_sm,'DecisionTree GridSearch SMOTE',Metrics)
Metrics



"""# Random Forest Classifier"""

rf=RandomForestClassifier()


X_train=X_original_train
X_test=X_original_test
y_train=y_original_train
y_test=y_original_test


rf.fit(X_train,y_train)

y_pred_train_rf = rf.predict(X_train)
y_pred_test_rf = rf.predict(X_test)

train_acc=rf.score(X_train,y_train)
test_acc=rf.score(X_test,y_test)

f1_score_train=f1_score(y_train,y_pred_train, average='weighted')

f1_score_test=f1_score(y_test,y_pred_test, average='weighted')

# Generate the classification report
report_train = classification_report(y_train, y_pred_train_rf)


# Generate the classification report
report_test = classification_report(y_test, y_pred_test_rf)


print('Train accuracy : ',train_acc)
print('')

print('Test accuracy : ',test_acc)
print('')

print('F1_score of Train data :',f1_score_train)
print('')

print('F1_score of Test data :',f1_score_test)
print('')
print('--------------------** TRAIN REPORT **----------------------------')
print('')
print('CLASSIFICATION REPORT TRAIN DATA :')
print('')
print(report_train)
print('--------------------** TEST REPORT **----------------------------')
print('')

print('CLASSIFICATION REPORT TEST DATA :')
print('')
print(report_test)
print('')
print('-----------------------------------------------------------------')

cm = confusion_matrix(y_pred_test_rf,y_test)
print(cm)

# generate ROC curve
fpr, tpr, thresholds = roc_curve(y_test,y_pred_test_rf)
roc_auc = auc(fpr, tpr)

# plot the ROC curve and AUC percentage
plt.plot(fpr, tpr, label='ROC curve (AUC = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

Metrics=get_metrics(y_train,y_pred_train_rf,y_test,y_pred_test_rf,'RandomForest',Metrics)
Metrics



"""# with SMOTE - (RFC)"""

rf=RandomForestClassifier()

X_train=X_smote_train
X_test=X_original_test
y_train=y_smote_train
y_test=y_original_test

rf.fit(X_train,y_train)

y_pred_train_rf_sm = rf.predict(X_train)
y_pred_test_rf_sm = rf.predict(X_test)

train_acc=rf.score(X_train,y_train)
test_acc=rf.score(X_test,y_test)

f1_score_train=f1_score(y_train,y_pred_train_rf_sm, average='weighted')

f1_score_test=f1_score(y_test,y_pred_test_rf_sm, average='weighted')

# Generate the classification report
report_train = classification_report(y_train, y_pred_train_rf_sm)


# Generate the classification report
report_test = classification_report(y_test, y_pred_test_rf_sm)


print('Train accuracy : ',train_acc)
print('')

print('Test accuracy : ',test_acc)
print('')

print('F1_score of Train data :',f1_score_train)
print('')

print('F1_score of Test data :',f1_score_test)
print('')
print('--------------------** TRAIN REPORT **----------------------------')
print('')
print('CLASSIFICATION REPORT TRAIN DATA :')
print('')
print(report_train)
print('--------------------** TEST REPORT **----------------------------')
print('')

print('CLASSIFICATION REPORT TEST DATA :')
print('')
print(report_test)
print('')
print('-----------------------------------------------------------------')

cm = confusion_matrix(y_pred_test_rf_sm,y_test)
print(cm)

# generate ROC curve
fpr, tpr, thresholds = roc_curve(y_test,y_pred_test_rf_sm)
roc_auc = auc(fpr, tpr)

# plot the ROC curve and AUC percentage
plt.plot(fpr, tpr, label='ROC curve (AUC = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

Metrics=get_metrics(y_train,y_pred_train_rf_sm,y_test,y_pred_test_rf_sm,'RandomForest SMOTE',Metrics)
Metrics



"""# GridSearch - (RFC)"""



param_grid = {'n_estimators': [10, 20, 50],
              'max_depth': [2, 5, 10, 15, 20],
              'min_samples_split': [2, 5, 10],
              'min_samples_leaf': [1, 2, 4],
              'bootstrap': [True, False]}

X_train=X_original_train
X_test=X_original_test
y_train=y_original_train
y_test=y_original_test

# using GridSearchCV to find the best hyperparameters
grid_search = GridSearchCV(rf, param_grid, cv=5)
grid_search.fit(X_train, y_train)

# printing the best hyperparameters
print("Best hyperparameters: ", grid_search.best_params_)
dt_params=RandomForestClassifier(**grid_search.best_params_)
dt_params.fit(X_train, y_train)

y_pred_train_rf_gs = grid_search.predict(X_train)
y_pred_test_rf_gs = grid_search.predict(X_test)

train_acc=grid_search.score(X_train,y_train)

test_acc=grid_search.score(X_test,y_test)

f1_score_train=f1_score(y_train,y_pred_train_rf_gs, average='weighted')

f1_score_test=f1_score(y_test,y_pred_test_rf_gs, average='weighted')

# Generate the classification report
report_train = classification_report(y_train, y_pred_train_rf_gs)


# Generate the classification report
report_test = classification_report(y_test, y_pred_test_rf_gs)


print('Train accuracy : ',train_acc)
print('')

print('Test accuracy : ',test_acc)
print('')

print('F1_score of Train data :',f1_score_train)
print('')

print('F1_score of Test data :',f1_score_test)
print('')
print('--------------------** TRAIN REPORT **----------------------------')
print('')
print('CLASSIFICATION REPORT TRAIN DATA :')
print('')
print(report_train)
print('--------------------** TEST REPORT **----------------------------')
print('')

print('CLASSIFICATION REPORT TEST DATA :')
print('')
print(report_test)
print('')
print('-----------------------------------------------------------------')

cm = confusion_matrix(y_pred_test_rf_gs,y_test)
print(cm)

# generate ROC curve
fpr, tpr, thresholds = roc_curve(y_test,y_pred_test_rf_gs)
roc_auc = auc(fpr, tpr)

# plot the ROC curve and AUC percentage
plt.plot(fpr, tpr, label='ROC curve (AUC = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

Metrics=get_metrics(y_train,y_pred_train_rf_gs,y_test,y_pred_test_rf_gs,'RandomForest GridSearch',Metrics)
Metrics



"""# GridSearchCV - With SMOTE (RFC)"""



param_grid = {'n_estimators': [10,20, 50],
              'max_depth': [2, 5, 10, 15, 20],
              'min_samples_split': [2, 5, 10],
              'min_samples_leaf': [1, 2, 4],
              'bootstrap': [True, False]}

X_train=X_smote_train
X_test=X_original_test
y_train=y_smote_train
y_test=y_original_test

# using GridSearchCV to find the best hyperparameters
grid_search = GridSearchCV(rf, param_grid, cv=5)
grid_search.fit(X_train, y_train)

# printing the best hyperparameters
print("Best hyperparameters: ", grid_search.best_params_)
dt_params=RandomForestClassifier(**grid_search.best_params_)
dt_params.fit(X_train, y_train)

rf=RandomForestClassifier()
X_train=X_smote_train
y_train=y_smote_train
rf.fit(X_train,y_train)

y_pred_train_rf_gs_sm = grid_search.predict(X_train)
y_pred_test_rf_gs_sm = grid_search.predict(X_test)

train_acc=grid_search.score(X_train,y_train)
test_acc=grid_search.score(X_test,y_test)

f1_score_train=f1_score(y_train,y_pred_train_rf_gs_sm, average='weighted')

f1_score_test=f1_score(y_test,y_pred_test_rf_gs_sm, average='weighted')

# Generate the classification report
report_train = classification_report(y_train, y_pred_train_rf_gs_sm)


# Generate the classification report
report_test = classification_report(y_test, y_pred_test_rf_gs_sm)


print('Train accuracy : ',train_acc)
print('')

print('Test accuracy : ',test_acc)
print('')

print('F1_score of Train data :',f1_score_train)
print('')

print('F1_score of Test data :',f1_score_test)
print('')
print('--------------------** TRAIN REPORT **----------------------------')
print('')
print('CLASSIFICATION REPORT TRAIN DATA :')
print('')
print(report_train)
print('--------------------** TEST REPORT **----------------------------')
print('')

print('CLASSIFICATION REPORT TEST DATA :')
print('')
print(report_test)
print('')
print('-----------------------------------------------------------------')

cm = confusion_matrix(y_pred_test_rf_gs_sm,y_test)
print(cm)

# generate ROC curve
fpr, tpr, thresholds = roc_curve(y_test,y_pred_test_rf_gs_sm)
roc_auc = auc(fpr, tpr)

# plot the ROC curve and AUC percentage
plt.plot(fpr, tpr, label='ROC curve (AUC = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

Metrics=get_metrics(y_train,y_pred_train_rf_gs_sm,y_test,y_pred_test_rf_gs_sm,'RandomForest GridSearch SMOTE',Metrics)
Metrics



"""# Xg BOOST CLASSIFIER"""

pip install xgboost

import xgboost as xgb
from xgboost.sklearn import XGBClassifier

xgb = xgb.XGBClassifier()

X_train=X_original_train
X_test=X_original_test
y_train=y_original_train
y_test=y_original_test

xgb.fit(X_train,y_train)

y_pred_train_xg = xgb.predict(X_train)
y_pred_test_xg = xgb.predict(X_test)

train_acc=xgb.score(X_train,y_train)
test_acc=xgb.score(X_test,y_test)

f1_score_train=f1_score(y_train,y_pred_train_xg, average='weighted')
f1_score_test=f1_score(y_test,y_pred_test_xg, average='weighted')

# Generate the classification report
report_train = classification_report(y_train, y_pred_train_xg)


# Generate the classification report
report_test = classification_report(y_test, y_pred_test_xg)


print('Train accuracy : ',train_acc)
print('')

print('Test accuracy : ',test_acc)
print('')

print('F1_score of Train data :',f1_score_train)
print('')

print('F1_score of Test data :',f1_score_test)
print('')
print('--------------------** TRAIN REPORT **----------------------------')
print('')
print('CLASSIFICATION REPORT TRAIN DATA :')
print('')
print(report_train)
print('--------------------** TEST REPORT **----------------------------')
print('')

print('CLASSIFICATION REPORT TEST DATA :')
print('')
print(report_test)
print('')
print('-----------------------------------------------------------------')

cm = confusion_matrix(y_pred_test_xg,y_test)
print(cm)

# generate ROC curve
fpr, tpr, thresholds = roc_curve(y_test,y_pred_test_xg)
roc_auc = auc(fpr, tpr)

# plot the ROC curve and AUC percentage
plt.plot(fpr, tpr, label='ROC curve (AUC = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

Metrics=get_metrics(y_train,y_pred_train_xg,y_test,y_pred_test_xg,'XGBOOST',Metrics)
Metrics



"""# with SMOTE -(XGB)"""

import xgboost as xgb
from xgboost.sklearn import XGBClassifier
xgb = xgb.XGBClassifier()

X_train=X_smote_train
X_test=X_original_test
y_train=y_smote_train
y_test=y_original_test


xgb.fit(X_train,y_train)

y_pred_train_xg_sm = xgb.predict(X_train)
y_pred_test_xg_sm = xgb.predict(X_test)

train_acc=xgb.score(X_train,y_train)
test_acc=xgb.score(X_test,y_test)

f1_score_train=f1_score(y_train,y_pred_train_xg_sm, average='weighted')

f1_score_test=f1_score(y_test,y_pred_test_xg_sm, average='weighted')

# Generate the classification report
report_train = classification_report(y_train, y_pred_train_xg_sm)


# Generate the classification report
report_test = classification_report(y_test, y_pred_test_xg_sm)


print('Train accuracy : ',train_acc)
print('')

print('Test accuracy : ',test_acc)
print('')

print('F1_score of Train data :',f1_score_train)
print('')

print('F1_score of Test data :',f1_score_test)
print('')
print('--------------------** TRAIN REPORT **----------------------------')
print('')
print('CLASSIFICATION REPORT TRAIN DATA :')
print('')
print(report_train)
print('--------------------** TEST REPORT **----------------------------')
print('')

print('CLASSIFICATION REPORT TEST DATA :')
print('')
print(report_test)
print('')
print('-----------------------------------------------------------------')

cm = confusion_matrix(y_pred_test_xg_sm,y_test)
print(cm)

# generate ROC curve
fpr, tpr, thresholds = roc_curve(y_test,y_pred_test_xg_sm)
roc_auc = auc(fpr, tpr)

# plot the ROC curve and AUC percentage
plt.plot(fpr, tpr, label='ROC curve (AUC = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

Metrics=get_metrics(y_train,y_pred_train_xg_sm,y_test,y_pred_test_xg_sm,'XGBOOST SMOTE',Metrics)
Metrics



"""# GridSearchCV (XGB)"""

param_grid = {'n_estimators': [10, 20, 50],
              'max_depth': [2, 5, 10],
              'learning_rate': [0.01, 0.1, 1],
              'subsample': [0.5, 0.7, 1],
              'colsample_bytree': [0.5, 0.7, 1],
              'gamma': [0, 0.1, 0.5]}

X_train=X_original_train
X_test=X_original_test
y_train=y_original_train
y_test=y_original_test

# using GridSearchCV to find the best hyperparameters
grid_search = GridSearchCV(xgb, param_grid, cv=5)
grid_search.fit(X_train, y_train)

# printing the best hyperparameters
print("Best hyperparameters: ", grid_search.best_params_)
grid_search=XGBClassifier(**grid_search.best_params_)
grid_search.fit(X_train, y_train)

y_pred_train_xg_gs = grid_search.predict(X_train)
y_pred_test_xg_gs = grid_search.predict(X_test)

train_acc=grid_search.score(X_train,y_train)

test_acc=grid_search.score(X_test,y_test)

f1_score_train=f1_score(y_train,y_pred_train_xg_gs, average='weighted')

f1_score_test=f1_score(y_test,y_pred_test_xg_gs, average='weighted')

# Generate the classification report
report_train = classification_report(y_train, y_pred_train_xg_gs)


# Generate the classification report
report_test = classification_report(y_test, y_pred_test_xg_gs)


print('Train accuracy : ',train_acc)
print('')

print('Test accuracy : ',test_acc)
print('')

print('F1_score of Train data :',f1_score_train)
print('')

print('F1_score of Test data :',f1_score_test)
print('')
print('--------------------** TRAIN REPORT **----------------------------')
print('')
print('CLASSIFICATION REPORT TRAIN DATA :')
print('')
print(report_train)
print('--------------------** TEST REPORT **----------------------------')
print('')

print('CLASSIFICATION REPORT TEST DATA :')
print('')
print(report_test)
print('')
print('-----------------------------------------------------------------')

cm = confusion_matrix(y_pred_test_xg_gs,y_test)
print(cm)

# generate ROC curve
fpr, tpr, thresholds = roc_curve(y_test,y_pred_test_xg_gs)
roc_auc = auc(fpr, tpr)

# plot the ROC curve and AUC percentage
plt.plot(fpr, tpr, label='ROC curve (AUC = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

Metrics=get_metrics(y_train,y_pred_train_xg_gs,y_test,y_pred_test_xg_gs,'XGBOOST GridSearch',Metrics)
Metrics

"""# GridSearchCV - With SMOTE (XGB)"""

xgb =XGBClassifier()
param_grid = {'n_estimators': [10, 20, 50],
              'max_depth': [2, 5, 10],
              'learning_rate': [0.01, 0.1, 1],
              'subsample': [0.5, 0.7, 1],
              'colsample_bytree': [0.5, 0.7, 1],
              'gamma': [0, 0.1, 0.5]}



X_train=X_smote_train
X_test=X_original_test
y_train=y_smote_train
y_test=y_original_test



# using GridSearchCV to find the best hyperparameters
grid_search = GridSearchCV(xgb, param_grid, cv=5)
grid_search.fit(X_train, y_train)

# printing the best hyperparameters
print("Best hyperparameters: ", grid_search.best_params_)
grid_search=XGBClassifier(**grid_search.best_params_)
grid_search.fit(X_train, y_train)

#xgb =XGBClassifier()
X_train=X_smote_train
y_train=y_smote_train
xgb.fit(X_train,y_train)

y_pred_train_xg_gs_sm = grid_search.predict(X_train)
y_pred_test_xg_gs_sm = grid_search.predict(X_test)

train_acc=grid_search.score(X_train,y_train)
test_acc=grid_search.score(X_test,y_test)

f1_score_train=f1_score(y_train,y_pred_train_xg_gs_sm, average='weighted')

f1_score_test=f1_score(y_test,y_pred_test_xg_gs_sm, average='weighted')

# Generate the classification report
report_train = classification_report(y_train, y_pred_train_xg_gs_sm)


# Generate the classification report
report_test = classification_report(y_test, y_pred_test_xg_gs_sm)


print('Train accuracy : ',train_acc)
print('')

print('Test accuracy : ',test_acc)
print('')

print('F1_score of Train data :',f1_score_train)
print('')

print('F1_score of Test data :',f1_score_test)
print('')
print('--------------------** TRAIN REPORT **----------------------------')
print('')
print('CLASSIFICATION REPORT TRAIN DATA :')
print('')
print(report_train)
print('--------------------** TEST REPORT **----------------------------')
print('')

print('CLASSIFICATION REPORT TEST DATA :')
print('')
print(report_test)
print('')
print('-----------------------------------------------------------------')

cm = confusion_matrix(y_pred_test_xg_gs_sm,y_test)
print(cm)

# generate ROC curve
fpr, tpr, thresholds = roc_curve(y_test,y_pred_test_xg_gs_sm)
roc_auc = auc(fpr, tpr)

# plot the ROC curve and AUC percentage
plt.plot(fpr, tpr, label='ROC curve (AUC = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

Metrics=get_metrics(y_train,y_pred_train_xg_gs_sm,y_test,y_pred_test_xg_gs_sm,'XGBOOST GridSearch SMOTE',Metrics)
Metrics

"""# SUPPORT VECTOR MACHINE CLASSIFIER  (SVC)"""

from sklearn.svm import SVC

svm=SVC()

X_train=X_original_train
X_test=X_original_test
y_train=y_original_train
y_test=y_original_test

svm.fit(X_train,y_train)

y_pred_train_svm = svm.predict(X_train)
y_pred_test_svm = svm.predict(X_test)

train_acc=svm.score(X_train,y_train)
test_acc=svm.score(X_test,y_test)

f1_score_train=f1_score(y_train,y_pred_train_svm, average='weighted')
f1_score_test=f1_score(y_test,y_pred_test_svm, average='weighted')

# Generate the classification report
report_train = classification_report(y_train, y_pred_train_svm)


# Generate the classification report
report_test = classification_report(y_test, y_pred_test_svm)


print('Train accuracy : ',train_acc)
print('')

print('Test accuracy : ',test_acc)
print('')

print('F1_score of Train data :',f1_score_train)
print('')
print('F1_score of Test data :',f1_score_test)
print('')
print('--------------------** TRAIN REPORT **----------------------------')
print('')
print('CLASSIFICATION REPORT TRAIN DATA :')
print('')
print(report_train)
print('--------------------** TEST REPORT **----------------------------')
print('')

print('CLASSIFICATION REPORT TEST DATA :')
print('')
print(report_test)
print('')
print('-----------------------------------------------------------------')

cm = confusion_matrix(y_pred_test_svm,y_test)
print(cm)

# generate ROC curve
fpr, tpr, thresholds = roc_curve(y_test,y_pred_test_svm)
roc_auc = auc(fpr, tpr)

# plot the ROC curve and AUC percentage
plt.plot(fpr, tpr, label='ROC curve (AUC = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

Metrics=get_metrics(y_train,y_pred_train_svm,y_test,y_pred_test_svm,'SVM',Metrics)
Metrics

"""# With SMOTE - (SVM)"""

svm=SVC()

X_train=X_smote_train
X_test=X_original_test
y_train=y_smote_train
y_test=y_original_test

svm.fit(X_train,y_train)

y_pred_train_svm_sm = svm.predict(X_train)
y_pred_test_svm_sm = svm.predict(X_test)

train_acc=svm.score(X_train,y_train)
test_acc=svm.score(X_test,y_test)

f1_score_train=f1_score(y_train,y_pred_train_svm_sm, average='weighted')
f1_score_test=f1_score(y_test,y_pred_test_svm_sm, average='weighted')

# Generate the classification report
report_train = classification_report(y_train, y_pred_train_svm_sm)


# Generate the classification report
report_test = classification_report(y_test, y_pred_test_svm_sm)


print('Train accuracy : ',train_acc)
print('')

print('Test accuracy : ',test_acc)
print('')

print('F1_score of Train data :',f1_score_train)
print('')
print('F1_score of Test data :',f1_score_test)
print('')
print('--------------------** TRAIN REPORT **----------------------------')
print('')
print('CLASSIFICATION REPORT TRAIN DATA :')
print('')
print(report_train)
print('--------------------** TEST REPORT **----------------------------')
print('')

print('CLASSIFICATION REPORT TEST DATA :')
print('')
print(report_test)
print('')
print('-----------------------------------------------------------------')

cm = confusion_matrix(y_pred_test_svm_sm,y_test)
print(cm)

# generate ROC curve
fpr, tpr, thresholds = roc_curve(y_test,y_pred_test_svm_sm)
roc_auc = auc(fpr, tpr)

# plot the ROC curve and AUC percentage
plt.plot(fpr, tpr, label='ROC curve (AUC = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

Metrics=get_metrics(y_train,y_pred_train_svm_sm,y_test,y_pred_test_svm_sm,'SVM SMOTE',Metrics)
Metrics

"""# KNN CLASSIFIER"""

from sklearn.neighbors import KNeighborsClassifier

knn=KNeighborsClassifier()

X_train=X_original_train
X_test=X_original_test
y_train=y_original_train
y_test=y_original_test

knn.fit(X_train,y_train)

y_pred_train_knn = knn.predict(X_train)
y_pred_test_knn = knn.predict(X_test)

train_acc=knn.score(X_train,y_train)
test_acc=knn.score(X_test,y_test)

f1_score_train=f1_score(y_train,y_pred_train_knn, average='weighted')

f1_score_test=f1_score(y_test,y_pred_test_knn, average='weighted')

# Generate the classification report
report_train = classification_report(y_train, y_pred_train_knn)


# Generate the classification report
report_test = classification_report(y_test, y_pred_test_knn)


print('Train accuracy : ',train_acc)
print('')

print('Test accuracy : ',test_acc)
print('')

print('F1_score of Train data :',f1_score_train)
print('')

print('F1_score of Test data :',f1_score_test)
print('')
print('--------------------** TRAIN REPORT **----------------------------')
print('')
print('CLASSIFICATION REPORT TRAIN DATA :')
print('')
print(report_train)
print('--------------------** TEST REPORT **----------------------------')
print('')

print('CLASSIFICATION REPORT TEST DATA :')
print('')
print(report_test)
print('')
print('-----------------------------------------------------------------')

cm = confusion_matrix(y_pred_test_knn,y_test)
print(cm)

# generate ROC curve
fpr, tpr, thresholds = roc_curve(y_test,y_pred_test_knn)
roc_auc = auc(fpr, tpr)

# plot the ROC curve and AUC percentage
plt.plot(fpr, tpr, label='ROC curve (AUC = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

Metrics=get_metrics(y_train,y_pred_train_knn,y_test,y_pred_test_knn,'KNN',Metrics)
Metrics

"""# With SMOTE - (KNN)"""

knn=KNeighborsClassifier()

X_train=X_smote_train
X_test=X_original_test
y_train=y_smote_train
y_test=y_original_test

knn.fit(X_train,y_train)

y_pred_train_knn_sm = knn.predict(X_train)
y_pred_test_knn_sm = knn.predict(X_test)

train_acc=knn.score(X_train,y_train)
test_acc=knn.score(X_test,y_test)

f1_score_train=f1_score(y_train,y_pred_train_knn_sm, average='weighted')

f1_score_test=f1_score(y_test,y_pred_test_knn_sm, average='weighted')

# Generate the classification report
report_train = classification_report(y_train, y_pred_train_knn_sm)


# Generate the classification report
report_test = classification_report(y_test, y_pred_test_knn_sm)


print('Train accuracy : ',train_acc)
print('')

print('Test accuracy : ',test_acc)
print('')

print('F1_score of Train data :',f1_score_train)
print('')

print('F1_score of Test data :',f1_score_test)
print('')
print('--------------------** TRAIN REPORT **----------------------------')
print('')
print('CLASSIFICATION REPORT TRAIN DATA :')
print('')
print(report_train)
print('--------------------** TEST REPORT **----------------------------')
print('')

print('CLASSIFICATION REPORT TEST DATA :')
print('')
print(report_test)
print('')
print('-----------------------------------------------------------------')

cm = confusion_matrix(y_pred_test_knn_sm,y_test)
print(cm)

# generate ROC curve
fpr, tpr, thresholds = roc_curve(y_test,y_pred_test_knn_sm)
roc_auc = auc(fpr, tpr)

# plot the ROC curve and AUC percentage
plt.plot(fpr, tpr, label='ROC curve (AUC = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

Metrics=get_metrics(y_train,y_pred_train_knn_sm,y_test,y_pred_test_knn_sm,'KNN SMOTE',Metrics)
Metrics











"""# Reading Test Data:"""

test_demo=pd.read_csv("Test_Demographics.csv", na_values=['NA'])
test_claim=pd.read_csv("Test_Claim.csv", na_values=['?','-5','MISSINGVALUE','MISSEDDATA'])
test_policy=pd.read_csv("Test_Policy.csv", na_values=['-1','MISSINGVAL','NA'])
test_vehicle= pd.read_csv("Train_Vehicle.csv", na_values=['???'])
test_vehicle = test_vehicle.pivot(index="CustomerID", columns="VehicleAttribute", values="VehicleAttributeDetails")

test = test_demo.merge(test_claim, on="CustomerID")
test = test.merge(test_policy, on="CustomerID")
test = test.merge(test_vehicle, on="CustomerID")



"""# Mearging DataSets"""













test

test.isnull().sum()

test.isnull().sum().sum()

test.dtypes

test.nunique()

test.drop(['CustomerID','InsuredZipCode','Country','IncidentAddress','InsurancePolicyNumber','VehicleID','DateOfIncident','DateOfPolicyCoverage'], axis=1, inplace=True)



def init():
    global model

    # AZUREML_MODEL_DIR is an environment variable created during deployment
    # The path "model" is the name of the registered model's folder
    model_path = os.path.join(os.environ["AZUREML_MODEL_DIR"], "model")

    model = load_model(model_path)

